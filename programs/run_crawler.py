#!/usr/bin/env python3
"""
ç»Ÿä¸€çˆ¬è™«æœåŠ¡ - DY-Interaction Crawler Service

åŠŸèƒ½ï¼š
    - å†å²çˆ¬è™«ï¼šçˆ¬å–æ‰€æœ‰è§†é¢‘çš„å†å²è¯„è®º
    - ç›‘æ§çˆ¬è™«ï¼šå®šæœŸç›‘æ§æ–°å¢è¯„è®º
    - æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼å’Œè°ƒåº¦ç­–ç•¥
    - æ™ºèƒ½å»é‡ï¼šå†å²çˆ¬è™«è‡ªåŠ¨è·³è¿‡å·²çˆ¬å–è´¦å·
    - äº¤äº’é€‰æ‹©ï¼šæ”¯æŒå•é€‰ã€å¤šé€‰ã€å…¨é€‰è´¦å·

ç”¨æ³•ï¼š
    # å†å²çˆ¬è™« - äº¤äº’å¼é€‰æ‹©ï¼ˆæ¨èï¼‰
    python programs/run_crawler.py history --interactive

    # å†å²çˆ¬è™« - è‡ªåŠ¨çˆ¬å–æ–°è´¦å·ï¼ˆé»˜è®¤ï¼‰
    python programs/run_crawler.py history

    # å†å²çˆ¬è™« - å¼ºåˆ¶çˆ¬å–æ‰€æœ‰è´¦å·
    python programs/run_crawler.py history --all

    # å†å²çˆ¬è™« - æŒ‡å®šè´¦å·
    python programs/run_crawler.py history --accounts 1,3

    # ç›‘æ§çˆ¬è™« - äº¤äº’å¼é€‰æ‹©
    python programs/run_crawler.py monitor --interactive

    # ç›‘æ§çˆ¬è™« - æ‰€æœ‰è´¦å·
    python programs/run_crawler.py monitor --all

    # æ··åˆæ¨¡å¼ - å…ˆå†å²åç›‘æ§
    python programs/run_crawler.py hybrid --interactive

æ™ºèƒ½ç‰¹æ€§ï¼š
    - å†å²çˆ¬è™«é»˜è®¤åªçˆ¬å–æ–°è´¦å·ï¼ˆæœªçˆ¬å–è¿‡çš„ï¼‰
    - äº¤äº’æ¨¡å¼æ˜¾ç¤ºæ¯ä¸ªè´¦å·çš„çˆ¬å–çŠ¶æ€ï¼ˆâœ“å·²çˆ¬å– / â­æ–°è´¦å·ï¼‰
    - é‡æ–°çˆ¬å–å·²çˆ¬è´¦å·æ—¶ä¼šäºŒæ¬¡ç¡®è®¤
    - æ”¯æŒé€‰æ‹© "n" å¿«é€Ÿé€‰æ‹©æ‰€æœ‰æ–°è´¦å·

è®¾è®¡ç†å¿µï¼š
    - å•ä¸€å…¥å£ï¼šæ‰€æœ‰çˆ¬è™«åŠŸèƒ½ç»Ÿä¸€ç®¡ç†
    - æ™ºèƒ½å»é‡ï¼šé¿å…é‡å¤çˆ¬å–æµªè´¹èµ„æº
    - ç”¨æˆ·å‹å¥½ï¼šäº¤äº’å¼é€‰æ‹©ï¼Œæ¸…æ™°çš„çŠ¶æ€æç¤º
    - å¯é…ç½®ï¼šæ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶
    - æ˜“æ‰©å±•ï¼šä¾¿äºæ·»åŠ æ–°çš„çˆ¬è™«ç­–ç•¥
"""

import sys
import logging
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import List, Optional

# è®¾ç½®é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_dir = PROJECT_ROOT / 'logs'
log_dir.mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'crawler_service.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

from src.database.manager import DatabaseManager
from src.crawler.history_crawler import HistoryCrawler
from src.crawler.monitor_crawler import MonitorCrawler
from src.generator.task_generator import TaskGenerator
from src.crawler.api_client import DouyinAPIClient
from src.crawler.server_pool import ServerPool
from src.crawler.parallel_crawler import ParallelCrawler


class CrawlerService:
    """ç»Ÿä¸€çš„çˆ¬è™«æœåŠ¡ç±»"""

    def __init__(self, enable_parallel: bool = False):
        """åˆå§‹åŒ–çˆ¬è™«æœåŠ¡

        Args:
            enable_parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œçˆ¬å–ï¼ˆéœ€è¦é…ç½®å¤šæœåŠ¡å™¨ï¼‰
        """
        self.db = None
        self.api_client = None
        self.task_generator = None
        self.config = {}
        self.enable_parallel = enable_parallel
        self.server_pool = None
        self.parallel_crawler = None

    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            # åŠ è½½é…ç½®
            logger.info("åŠ è½½é…ç½®...")
            self.config = self._load_config()

            # åˆå§‹åŒ–æ•°æ®åº“
            logger.info("åˆå§‹åŒ–æ•°æ®åº“...")
            self.db = DatabaseManager()
            self.db.init_db()
            logger.info("âœ“ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

            # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
            logger.info("åˆå§‹åŒ–APIå®¢æˆ·ç«¯...")
            self.api_client = DouyinAPIClient()
            logger.info("âœ“ APIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

            # åˆå§‹åŒ–ä»»åŠ¡ç”Ÿæˆå™¨
            self.task_generator = TaskGenerator(self.db)
            logger.info("âœ“ ä»»åŠ¡ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

            # åˆå§‹åŒ–æœåŠ¡å™¨æ± å’Œå¹¶è¡Œçˆ¬è™«ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_parallel:
                logger.info("åˆå§‹åŒ–æœåŠ¡å™¨æ± ...")
                self.server_pool = ServerPool()
                self.parallel_crawler = ParallelCrawler(self.server_pool)
                max_workers = self.server_pool.get_max_workers()
                logger.info(f"âœ“ å¹¶è¡Œçˆ¬å–å·²å¯ç”¨ (æœ€å¤§å¹¶å‘: {max_workers})")

            return True

        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error("""
å¯èƒ½çš„åŸå› :
  1. é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: config/config.json
  2. APIæœåŠ¡å™¨æ— æ³•è¿æ¥
  3. æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥

è§£å†³æ–¹æ¡ˆ:
  1. æ£€æŸ¥ config/config.json é…ç½®æ–‡ä»¶
  2. éªŒè¯APIæœåŠ¡å™¨åœ°å€å’Œå¯†é’¥
  3. ç¡®ä¿æ•°æ®åº“æ–‡ä»¶å¯è®¿é—®
            """)
            return False

    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶

        Returns:
            dict: é…ç½®å­—å…¸
        """
        try:
            config_file = PROJECT_ROOT / 'config' / 'config.json'
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning("âš  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
        except Exception as e:
            logger.warning(f"âš  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    def get_target_accounts(self, account_filter: Optional[str] = None, interactive: bool = False, mode: str = 'history') -> List:
        """è·å–ç›®æ ‡è´¦å·åˆ—è¡¨

        Args:
            account_filter: è´¦å·è¿‡æ»¤å™¨ï¼ˆå¦‚ "1,3" æˆ– "all"ï¼‰
            interactive: æ˜¯å¦ä½¿ç”¨äº¤äº’å¼é€‰æ‹©
            mode: çˆ¬è™«æ¨¡å¼ (history/monitor)ï¼Œç”¨äºæ™ºèƒ½è¿‡æ»¤

        Returns:
            List: ç›®æ ‡è´¦å·åˆ—è¡¨
        """
        all_accounts = self.db.get_target_accounts()

        if not all_accounts:
            logger.error("âŒ æœªé…ç½®ä»»ä½•ç›®æ ‡è´¦å·")
            logger.info("è¯·åœ¨ config/target_accounts.json ä¸­é…ç½®ç›®æ ‡è´¦å·")
            return []

        logger.info(f"âœ“ æ‰¾åˆ° {len(all_accounts)} ä¸ªç›®æ ‡è´¦å·")

        # å¯¹äºå†å²çˆ¬è™«ï¼Œæ£€æŸ¥å“ªäº›è´¦å·å·²ç»çˆ¬å–è¿‡
        crawled_accounts = set()
        if mode == 'history':
            crawled_accounts = self._get_crawled_accounts()

        # äº¤äº’å¼é€‰æ‹©
        if interactive:
            return self._interactive_select_accounts(all_accounts, crawled_accounts, mode)

        # å¦‚æœæ²¡æœ‰è¿‡æ»¤å™¨
        if not account_filter:
            # å†å²çˆ¬è™«é»˜è®¤åªçˆ¬å–æ–°è´¦å·
            if mode == 'history' and crawled_accounts:
                new_accounts = [acc for acc in all_accounts if acc.id not in crawled_accounts]
                if new_accounts:
                    logger.info(f"âœ“ å‘ç° {len(new_accounts)} ä¸ªæ–°è´¦å·ï¼ˆæœªçˆ¬å–è¿‡ï¼‰")
                    logger.info(f"   å·²è·³è¿‡ {len(crawled_accounts)} ä¸ªå·²çˆ¬å–è´¦å·")
                    logger.info("   æç¤ºï¼šä½¿ç”¨ --interactive å¯ä»¥é€‰æ‹©é‡æ–°çˆ¬å–")
                    return new_accounts
                else:
                    logger.warning("âš  æ‰€æœ‰è´¦å·éƒ½å·²çˆ¬å–è¿‡")
                    logger.info("   ä½¿ç”¨ --interactive å¯ä»¥é€‰æ‹©é‡æ–°çˆ¬å–")
                    return []
            # ç›‘æ§çˆ¬è™«é»˜è®¤å…¨éƒ¨
            return all_accounts

        # è§£æ 'all' è¿‡æ»¤å™¨
        if account_filter == 'all':
            return all_accounts

        # è§£æè´¦å·ç¼–å·è¿‡æ»¤
        try:
            indices = [int(x.strip()) for x in account_filter.split(',')]
            if any(i < 1 or i > len(all_accounts) for i in indices):
                logger.error(f"âŒ ç¼–å·è¶…å‡ºèŒƒå›´ï¼è¯·è¾“å…¥ 1-{len(all_accounts)} ä¹‹é—´çš„æ•°å­—")
                return []

            selected = [all_accounts[i-1] for i in sorted(set(indices))]
            logger.info(f"âœ“ å·²é€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected

        except ValueError:
            logger.error("âŒ è´¦å·ç¼–å·æ ¼å¼é”™è¯¯ï¼ç¤ºä¾‹ï¼š1,3")
            return []

    def _get_crawled_accounts(self) -> set:
        """è·å–å·²ç»çˆ¬å–è¿‡çš„è´¦å·IDé›†åˆ

        Returns:
            set: å·²çˆ¬å–è´¦å·çš„IDé›†åˆ
        """
        from src.database.models import Comment
        with self.db.session_scope() as session:
            # æŸ¥è¯¢æœ‰è¯„è®ºæ•°æ®çš„è´¦å·
            result = session.query(Comment.target_account_id).distinct().all()
            return {row[0] for row in result if row[0]}

    def _interactive_select_accounts(self, all_accounts: List, crawled_accounts: set, mode: str) -> List:
        """äº¤äº’å¼é€‰æ‹©è´¦å·

        Args:
            all_accounts: æ‰€æœ‰è´¦å·åˆ—è¡¨
            crawled_accounts: å·²çˆ¬å–è´¦å·IDé›†åˆ
            mode: çˆ¬è™«æ¨¡å¼

        Returns:
            List: é€‰æ‹©çš„è´¦å·åˆ—è¡¨
        """
        print("\n" + "=" * 70)
        print(f"ğŸ“‹ è´¦å·åˆ—è¡¨ï¼ˆå…± {len(all_accounts)} ä¸ªï¼‰")
        print("=" * 70)

        # æ˜¾ç¤ºè´¦å·åˆ—è¡¨
        print(f"\n{'ç¼–å·':<6} {'è´¦å·å':<25} {'æŠ–éŸ³ID':<20} {'çŠ¶æ€':<10}")
        print("-" * 70)

        for idx, acc in enumerate(all_accounts, 1):
            status = ""
            if mode == 'history':
                if acc.id in crawled_accounts:
                    status = "âœ“ å·²çˆ¬å–"
                else:
                    status = "â­ æ–°è´¦å·"

            print(f"{idx:<6} {acc.account_name:<25} {acc.account_id:<20} {status:<10}")

        print("\n" + "=" * 70)
        print("é€‰æ‹©æ–¹å¼:")
        print("  0 - å…¨éƒ¨è´¦å·")
        if mode == 'history' and crawled_accounts:
            new_count = len([a for a in all_accounts if a.id not in crawled_accounts])
            print(f"  n - ä»…æ–°è´¦å·ï¼ˆ{new_count}ä¸ªæœªçˆ¬å–ï¼‰")
        print("  1-{} - å•ä¸ªè´¦å·".format(len(all_accounts)))
        print("  1,3,5 - å¤šä¸ªè´¦å·ï¼ˆé€—å·åˆ†éš”ï¼‰")
        print("=" * 70)

        choice = input("\nè¯·è¾“å…¥é€‰æ‹©: ").strip()

        # å¤„ç†é€‰æ‹©
        if choice == '0':
            logger.info(f"âœ“ å·²é€‰æ‹©å…¨éƒ¨ {len(all_accounts)} ä¸ªè´¦å·")
            return all_accounts

        if mode == 'history' and choice.lower() == 'n':
            new_accounts = [acc for acc in all_accounts if acc.id not in crawled_accounts]
            if new_accounts:
                logger.info(f"âœ“ å·²é€‰æ‹© {len(new_accounts)} ä¸ªæ–°è´¦å·")
                return new_accounts
            else:
                logger.warning("âš  æ²¡æœ‰æ–°è´¦å·")
                return []

        # è§£æç¼–å·
        try:
            indices = [int(x.strip()) for x in choice.split(',')]
            if any(i < 1 or i > len(all_accounts) for i in indices):
                logger.error(f"âŒ ç¼–å·è¶…å‡ºèŒƒå›´ï¼")
                return []

            selected = [all_accounts[i-1] for i in sorted(set(indices))]
            logger.info(f"âœ“ å·²é€‰æ‹© {len(selected)} ä¸ªè´¦å·:")
            for acc in selected:
                status_mark = "âœ“" if acc.id in crawled_accounts else "â­"
                logger.info(f"   {status_mark} {acc.account_name}")

            # å¦‚æœé€‰æ‹©äº†å·²çˆ¬å–çš„è´¦å·ï¼Œæç¤ºç¡®è®¤
            if mode == 'history':
                selected_crawled = [acc for acc in selected if acc.id in crawled_accounts]
                if selected_crawled:
                    print(f"\nâš ï¸  æ³¨æ„ï¼šé€‰æ‹©ä¸­åŒ…å« {len(selected_crawled)} ä¸ªå·²çˆ¬å–è´¦å·ï¼Œå°†é‡æ–°çˆ¬å–")
                    confirm = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
                    if confirm != 'y':
                        logger.info("å·²å–æ¶ˆ")
                        return []

            return selected

        except ValueError:
            logger.error("âŒ è¾“å…¥æ ¼å¼é”™è¯¯")
            return []

    def run_history_crawler(self, accounts: List, days: int = 90) -> dict:
        """è¿è¡Œå†å²çˆ¬è™«

        Args:
            accounts: ç›®æ ‡è´¦å·åˆ—è¡¨
            days: çˆ¬å–æœ€è¿‘Nå¤©çš„æ•°æ®

        Returns:
            dict: çˆ¬è™«ç»“æœç»Ÿè®¡
        """
        logger.info("=" * 70)
        logger.info("ğŸ“ å†å²çˆ¬è™«æ¨¡å¼ - å¯åŠ¨")
        logger.info(f"   è´¦å·æ•°é‡: {len(accounts)}")
        logger.info(f"   æ—¶é—´èŒƒå›´: æœ€è¿‘ {days} å¤©")
        if self.enable_parallel:
            logger.info(f"   å¹¶è¡Œæ¨¡å¼: âœ“ å·²å¯ç”¨ (æœ€å¤§å¹¶å‘: {self.server_pool.get_max_workers()})")
        logger.info("=" * 70)

        # ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
        if self.enable_parallel and self.parallel_crawler:
            return self._run_history_parallel(accounts, days)

        # ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        return self._run_history_serial(accounts, days)

    def _run_history_serial(self, accounts: List, days: int = 90) -> dict:
        """ä¸²è¡Œæ¨¡å¼ï¼šé€ä¸ªè´¦å·çˆ¬å–ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        crawler = HistoryCrawler(self.db, self.api_client)

        total_comments = 0
        total_videos = 0
        total_tasks = 0
        success_count = 0
        failed_count = 0

        for idx, account in enumerate(accounts, 1):
            logger.info(f"\n[è´¦å· {idx}/{len(accounts)}] {account.account_name}")
            logger.info("-" * 70)

            try:
                # çˆ¬å–å†å²è¯„è®º
                result = crawler.crawl_history(account, days=days)

                if result['status'] == 'success':
                    videos = result.get('total_videos', 0)
                    comments = result.get('total_comments', 0)

                    logger.info(f"  âœ“ çˆ¬è™«å®Œæˆ")
                    logger.info(f"    - è§†é¢‘æ•°: {videos}")
                    logger.info(f"    - è¯„è®ºæ•°: {comments}")

                    total_videos += videos
                    total_comments += comments
                    success_count += 1

                    # ç”Ÿæˆä»»åŠ¡
                    task_count = self.task_generator.generate_from_history(account.id)
                    total_tasks += task_count
                    logger.info(f"  âœ“ ä»»åŠ¡ç”Ÿæˆ: {task_count} ä¸ª")

                else:
                    logger.error(f"  âœ— çˆ¬è™«å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    failed_count += 1

            except Exception as e:
                logger.error(f"  âœ— å¤„ç†å¤±è´¥: {e}")
                failed_count += 1

        # è¿”å›ç»Ÿè®¡ç»“æœ
        stats = {
            'mode': 'history',
            'accounts_total': len(accounts),
            'accounts_success': success_count,
            'accounts_failed': failed_count,
            'total_videos': total_videos,
            'total_comments': total_comments,
            'total_tasks': total_tasks
        }

        self._print_summary(stats)
        return stats

    def _run_history_parallel(self, accounts: List, days: int = 90) -> dict:
        """å¹¶è¡Œæ¨¡å¼ï¼šå¤šæœåŠ¡å™¨å¹¶å‘çˆ¬å–"""
        def crawl_one_account(account, server, db):
            """å¹¶è¡Œçˆ¬å–å•ä¸ªè´¦å·çš„åŒ…è£…å‡½æ•°"""
            try:
                # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä½¿ç”¨æœåŠ¡å™¨çš„cookieï¼‰
                from src.crawler.api_client import DouyinAPIClient
                api_client = DouyinAPIClient(cookie=server.cookie, api_url=server.url)
                crawler = HistoryCrawler(db, api_client)

                # çˆ¬å–å†å²
                result = crawler.crawl_history(account, days=days)

                if result['status'] == 'success':
                    # ç”Ÿæˆä»»åŠ¡
                    task_gen = TaskGenerator(db)
                    task_count = task_gen.generate_from_history(account.id)
                    logger.info(f"  [{server.name}] âœ“ ä»»åŠ¡ç”Ÿæˆ: {task_count} ä¸ª")
                    return True
                else:
                    logger.error(f"  [{server.name}] âœ— çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    return False

            except Exception as e:
                logger.error(f"  [{server.name}] âœ— å¼‚å¸¸: {e}")
                return False

        # ä½¿ç”¨å¹¶è¡Œçˆ¬è™«æ‰§è¡Œ
        result_stats = self.parallel_crawler.crawl_accounts(
            accounts=accounts,
            crawl_func=crawl_one_account,
            show_progress=True
        )

        # è½¬æ¢ç»Ÿè®¡æ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
        stats = {
            'mode': 'history_parallel',
            'accounts_total': result_stats['total'],
            'accounts_success': result_stats['successful'],
            'accounts_failed': result_stats['failed'],
            'total_videos': 0,  # å¹¶è¡Œæ¨¡å¼æš‚ä¸ç»Ÿè®¡
            'total_comments': 0,  # å¹¶è¡Œæ¨¡å¼æš‚ä¸ç»Ÿè®¡
            'total_tasks': 0,  # å¹¶è¡Œæ¨¡å¼æš‚ä¸ç»Ÿè®¡
            'duration': result_stats['duration'],
            'avg_time': result_stats['avg_time_per_task']
        }

        return stats

    def run_monitor_crawler(self, accounts: List, top_n: int = 5) -> dict:
        """è¿è¡Œç›‘æ§çˆ¬è™«

        Args:
            accounts: ç›®æ ‡è´¦å·åˆ—è¡¨
            top_n: ç›‘æ§å‰Nä¸ªè§†é¢‘

        Returns:
            dict: ç›‘æ§ç»“æœç»Ÿè®¡
        """
        logger.info("=" * 70)
        logger.info("ğŸ‘ï¸  ç›‘æ§çˆ¬è™«æ¨¡å¼ - å¯åŠ¨")
        logger.info(f"   è´¦å·æ•°é‡: {len(accounts)}")
        logger.info(f"   ç›‘æ§ç­–ç•¥: æ¯è´¦å·å‰ {top_n} ä¸ªè§†é¢‘")
        logger.info(f"   è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if self.enable_parallel:
            logger.info(f"   å¹¶è¡Œæ¨¡å¼: âœ“ å·²å¯ç”¨ (æœ€å¤§å¹¶å‘: {self.server_pool.get_max_workers()})")
        logger.info("=" * 70)

        # ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
        if self.enable_parallel and self.parallel_crawler:
            return self._run_monitor_parallel(accounts, top_n)

        # ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        return self._run_monitor_serial(accounts, top_n)

    def _run_monitor_serial(self, accounts: List, top_n: int = 5) -> dict:
        """ä¸²è¡Œæ¨¡å¼ï¼šé€ä¸ªè´¦å·ç›‘æ§ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        crawler = MonitorCrawler(self.db, self.api_client)

        total_new_comments = 0
        total_tasks = 0
        success_count = 0
        failed_count = 0

        for idx, account in enumerate(accounts, 1):
            logger.info(f"\n[è´¦å· {idx}/{len(accounts)}] {account.account_name}")
            logger.info("-" * 70)

            try:
                # ç›‘æ§æ–°å¢è¯„è®º
                result = crawler.monitor_daily(account, top_n=top_n)

                if result['status'] == 'success':
                    new_count = result.get('new_comments_count', 0)
                    logger.info(f"  âœ“ ç›‘æ§å®Œæˆ")
                    logger.info(f"    - æ–°å¢è¯„è®º: {new_count} æ¡")

                    total_new_comments += new_count
                    success_count += 1

                    # ç”Ÿæˆé«˜ä¼˜å…ˆçº§ä»»åŠ¡
                    if new_count > 0:
                        task_count = self.task_generator.generate_from_realtime(account.id)
                        total_tasks += task_count
                        logger.info(f"  âœ“ ç”Ÿæˆä»»åŠ¡: {task_count} ä¸ªï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰")
                    else:
                        logger.info(f"  â„¹ï¸  æš‚æ— æ–°å¢")

                else:
                    logger.error(f"  âœ— ç›‘æ§å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    failed_count += 1

            except Exception as e:
                logger.error(f"  âœ— å¤„ç†å¤±è´¥: {e}")
                failed_count += 1

        # è¿”å›ç»Ÿè®¡ç»“æœ
        stats = {
            'mode': 'monitor',
            'accounts_total': len(accounts),
            'accounts_success': success_count,
            'accounts_failed': failed_count,
            'new_comments': total_new_comments,
            'total_tasks': total_tasks
        }

        self._print_summary(stats)
        return stats

    def _run_monitor_parallel(self, accounts: List, top_n: int = 5) -> dict:
        """å¹¶è¡Œæ¨¡å¼ï¼šå¤šæœåŠ¡å™¨å¹¶å‘ç›‘æ§"""
        def monitor_one_account(account, server, db):
            """å¹¶è¡Œç›‘æ§å•ä¸ªè´¦å·çš„åŒ…è£…å‡½æ•°"""
            try:
                # åˆ›å»ºç›‘æ§çˆ¬è™«å®ä¾‹ï¼ˆä½¿ç”¨æœåŠ¡å™¨çš„cookieï¼‰
                from src.crawler.api_client import DouyinAPIClient
                api_client = DouyinAPIClient(cookie=server.cookie, api_url=server.url)
                crawler = MonitorCrawler(db, api_client)

                # ç›‘æ§
                result = crawler.monitor_daily(account, top_n=top_n)

                if result['status'] == 'success':
                    new_count = result.get('new_comments_count', 0)

                    # ç”Ÿæˆé«˜ä¼˜å…ˆçº§ä»»åŠ¡
                    if new_count > 0:
                        task_gen = TaskGenerator(db)
                        task_count = task_gen.generate_from_realtime(account.id)
                        logger.info(f"  [{server.name}] âœ“ æ–°å¢ {new_count} æ¡è¯„è®ºï¼Œç”Ÿæˆ {task_count} ä¸ªä»»åŠ¡")
                    else:
                        logger.info(f"  [{server.name}] âœ“ æš‚æ— æ–°å¢è¯„è®º")

                    return True
                else:
                    logger.error(f"  [{server.name}] âœ— ç›‘æ§å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    return False

            except Exception as e:
                logger.error(f"  [{server.name}] âœ— å¼‚å¸¸: {e}")
                return False

        # ä½¿ç”¨å¹¶è¡Œçˆ¬è™«æ‰§è¡Œ
        result_stats = self.parallel_crawler.crawl_accounts(
            accounts=accounts,
            crawl_func=monitor_one_account,
            show_progress=True
        )

        # è½¬æ¢ç»Ÿè®¡æ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
        stats = {
            'mode': 'monitor_parallel',
            'accounts_total': result_stats['total'],
            'accounts_success': result_stats['successful'],
            'accounts_failed': result_stats['failed'],
            'new_comments': 0,  # å¹¶è¡Œæ¨¡å¼æš‚ä¸ç»Ÿè®¡
            'total_tasks': 0,  # å¹¶è¡Œæ¨¡å¼æš‚ä¸ç»Ÿè®¡
            'duration': result_stats['duration'],
            'avg_time': result_stats['avg_time_per_task']
        }

        return stats

    def run_hybrid_crawler(self, accounts: List, days: int = 90, top_n: int = 5) -> dict:
        """è¿è¡Œæ··åˆæ¨¡å¼ï¼ˆå…ˆå†å²åç›‘æ§ï¼‰

        Args:
            accounts: ç›®æ ‡è´¦å·åˆ—è¡¨
            days: å†å²çˆ¬è™«å¤©æ•°
            top_n: ç›‘æ§å‰Nä¸ªè§†é¢‘

        Returns:
            dict: æ··åˆç»“æœç»Ÿè®¡
        """
        logger.info("=" * 70)
        logger.info("ğŸ”„ æ··åˆçˆ¬è™«æ¨¡å¼ - å¯åŠ¨")
        logger.info("   æ‰§è¡Œé¡ºåº: å†å²çˆ¬è™« â†’ ç›‘æ§çˆ¬è™«")
        logger.info("=" * 70)

        # å…ˆè¿è¡Œå†å²çˆ¬è™«
        logger.info("\nã€ç¬¬1é˜¶æ®µã€‘å†å²çˆ¬è™«")
        history_stats = self.run_history_crawler(accounts, days)

        # å†è¿è¡Œç›‘æ§çˆ¬è™«
        logger.info("\nã€ç¬¬2é˜¶æ®µã€‘ç›‘æ§çˆ¬è™«")
        monitor_stats = self.run_monitor_crawler(accounts, top_n)

        # åˆå¹¶ç»Ÿè®¡
        stats = {
            'mode': 'hybrid',
            'history': history_stats,
            'monitor': monitor_stats
        }

        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š æ··åˆæ¨¡å¼ - æ€»ä½“ç»Ÿè®¡")
        logger.info("=" * 70)
        logger.info(f"  å†å²è¯„è®ºæ•°: {history_stats['total_comments']}")
        logger.info(f"  æ–°å¢è¯„è®ºæ•°: {monitor_stats['new_comments']}")
        logger.info(f"  æ€»ä»»åŠ¡æ•°: {history_stats['total_tasks'] + monitor_stats['total_tasks']}")
        logger.info("=" * 70)

        return stats

    def _print_summary(self, stats: dict):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦

        Args:
            stats: ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        logger.info("\n" + "=" * 70)
        logger.info(f"ğŸ“Š {stats['mode'].upper()} æ¨¡å¼ - ç»Ÿè®¡ç»“æœ")
        logger.info("=" * 70)
        logger.info(f"  è´¦å·æ€»æ•°: {stats['accounts_total']}")
        logger.info(f"    - æˆåŠŸ: {stats['accounts_success']}")
        logger.info(f"    - å¤±è´¥: {stats['accounts_failed']}")

        if stats['mode'] == 'history':
            logger.info(f"  æ€»è§†é¢‘æ•°: {stats['total_videos']}")
            logger.info(f"  æ€»è¯„è®ºæ•°: {stats['total_comments']}")
            logger.info(f"  ç”Ÿæˆä»»åŠ¡: {stats['total_tasks']}")
        elif stats['mode'] == 'monitor':
            logger.info(f"  æ–°å¢è¯„è®º: {stats['new_comments']}")
            logger.info(f"  ç”Ÿæˆä»»åŠ¡: {stats['total_tasks']}")

        logger.info("=" * 70)


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='DY-Interaction ç»Ÿä¸€çˆ¬è™«æœåŠ¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å†å²çˆ¬è™« - äº¤äº’å¼é€‰æ‹©è´¦å·ï¼ˆæ¨èï¼‰
  python programs/run_crawler.py history --interactive

  # å†å²çˆ¬è™« - è‡ªåŠ¨çˆ¬å–æ–°è´¦å·ï¼ˆé»˜è®¤ï¼‰
  python programs/run_crawler.py history

  # å†å²çˆ¬è™« - çˆ¬å–æ‰€æœ‰è´¦å·ï¼ˆåŒ…æ‹¬å·²çˆ¬å–ï¼‰
  python programs/run_crawler.py history --all

  # å†å²çˆ¬è™« - çˆ¬å–æŒ‡å®šè´¦å·
  python programs/run_crawler.py history --accounts 1,3

  # å†å²çˆ¬è™« - å¹¶è¡Œæ¨¡å¼ï¼ˆå¤šæœåŠ¡å™¨å¹¶å‘ï¼‰ğŸš€
  python programs/run_crawler.py history --all --parallel

  # å†å²çˆ¬è™« - äº¤äº’é€‰æ‹© + å¹¶è¡Œæ¨¡å¼
  python programs/run_crawler.py history --interactive --parallel

  # ç›‘æ§çˆ¬è™« - äº¤äº’å¼é€‰æ‹©è´¦å·
  python programs/run_crawler.py monitor --interactive

  # ç›‘æ§çˆ¬è™« - ç›‘æ§æ‰€æœ‰è´¦å·
  python programs/run_crawler.py monitor --all

  # ç›‘æ§çˆ¬è™« - å¹¶è¡Œæ¨¡å¼
  python programs/run_crawler.py monitor --all --parallel

  # æ··åˆæ¨¡å¼ - å…ˆå†å²åç›‘æ§
  python programs/run_crawler.py hybrid --interactive

  # æ··åˆæ¨¡å¼ - å¹¶è¡Œçˆ¬å–
  python programs/run_crawler.py hybrid --all --parallel
        """
    )

    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='mode', help='çˆ¬è™«æ¨¡å¼', required=True)

    # å†å²çˆ¬è™«æ¨¡å¼
    history_parser = subparsers.add_parser('history', help='å†å²çˆ¬è™«æ¨¡å¼')
    history_parser.add_argument('--all', action='store_true', help='çˆ¬å–æ‰€æœ‰è´¦å·')
    history_parser.add_argument('--accounts', type=str, help='æŒ‡å®šè´¦å·ç¼–å·ï¼ˆå¦‚ï¼š1,3ï¼‰')
    history_parser.add_argument('--interactive', '-i', action='store_true', help='äº¤äº’å¼é€‰æ‹©è´¦å·')
    history_parser.add_argument('--days', type=int, default=90, help='çˆ¬å–æœ€è¿‘Nå¤©ï¼ˆé»˜è®¤90ï¼‰')
    history_parser.add_argument('--parallel', '-p', action='store_true', help='å¯ç”¨å¹¶è¡Œçˆ¬å–ï¼ˆéœ€é…ç½®å¤šæœåŠ¡å™¨ï¼‰')

    # ç›‘æ§çˆ¬è™«æ¨¡å¼
    monitor_parser = subparsers.add_parser('monitor', help='ç›‘æ§çˆ¬è™«æ¨¡å¼')
    monitor_parser.add_argument('--all', action='store_true', help='ç›‘æ§æ‰€æœ‰è´¦å·ï¼ˆé»˜è®¤ï¼‰')
    monitor_parser.add_argument('--accounts', type=str, help='æŒ‡å®šè´¦å·ç¼–å·ï¼ˆå¦‚ï¼š1,3ï¼‰')
    monitor_parser.add_argument('--interactive', '-i', action='store_true', help='äº¤äº’å¼é€‰æ‹©è´¦å·')
    monitor_parser.add_argument('--top-n', type=int, default=5, help='ç›‘æ§å‰Nä¸ªè§†é¢‘ï¼ˆé»˜è®¤5ï¼‰')
    monitor_parser.add_argument('--parallel', '-p', action='store_true', help='å¯ç”¨å¹¶è¡Œçˆ¬å–ï¼ˆéœ€é…ç½®å¤šæœåŠ¡å™¨ï¼‰')

    # æ··åˆæ¨¡å¼
    hybrid_parser = subparsers.add_parser('hybrid', help='æ··åˆæ¨¡å¼ï¼ˆå†å²+ç›‘æ§ï¼‰')
    hybrid_parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰è´¦å·')
    hybrid_parser.add_argument('--accounts', type=str, help='æŒ‡å®šè´¦å·ç¼–å·ï¼ˆå¦‚ï¼š1,3ï¼‰')
    hybrid_parser.add_argument('--interactive', '-i', action='store_true', help='äº¤äº’å¼é€‰æ‹©è´¦å·')
    hybrid_parser.add_argument('--days', type=int, default=90, help='å†å²çˆ¬è™«å¤©æ•°ï¼ˆé»˜è®¤90ï¼‰')
    hybrid_parser.add_argument('--top-n', type=int, default=5, help='ç›‘æ§è§†é¢‘æ•°ï¼ˆé»˜è®¤5ï¼‰')
    hybrid_parser.add_argument('--parallel', '-p', action='store_true', help='å¯ç”¨å¹¶è¡Œçˆ¬å–ï¼ˆéœ€é…ç½®å¤šæœåŠ¡å™¨ï¼‰')

    return parser


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = create_parser()
    args = parser.parse_args()

    logger.info("=" * 70)
    logger.info("ğŸš€ DY-Interaction ç»Ÿä¸€çˆ¬è™«æœåŠ¡")
    logger.info(f"   æ¨¡å¼: {args.mode.upper()}")
    logger.info(f"   å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)

    try:
        # åˆå§‹åŒ–æœåŠ¡
        enable_parallel = hasattr(args, 'parallel') and args.parallel
        service = CrawlerService(enable_parallel=enable_parallel)
        if not service.initialize():
            return 1

        # è·å–ç›®æ ‡è´¦å·
        account_filter = 'all' if args.all else args.accounts
        interactive = getattr(args, 'interactive', False)
        accounts = service.get_target_accounts(account_filter, interactive, args.mode)

        if not accounts:
            logger.error("âŒ æ²¡æœ‰å¯å¤„ç†çš„è´¦å·")
            return 1

        # æ ¹æ®æ¨¡å¼æ‰§è¡Œçˆ¬è™«
        if args.mode == 'history':
            service.run_history_crawler(accounts, args.days)

        elif args.mode == 'monitor':
            service.run_monitor_crawler(accounts, args.top_n)

        elif args.mode == 'hybrid':
            service.run_hybrid_crawler(accounts, args.days, args.top_n)

        logger.info("\nâœ… çˆ¬è™«æœåŠ¡æ‰§è¡Œå®Œæˆ")
        return 0

    except KeyboardInterrupt:
        logger.info("\n\nâ¸ï¸  æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        return 0

    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)
