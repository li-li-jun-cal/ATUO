#!/usr/bin/env python3
"""
ç»Ÿä¸€çˆ¬è™«æœåŠ¡ - DY-Interaction Crawler Service

åŠŸèƒ½ï¼š
    - å†å²çˆ¬è™«ï¼šçˆ¬å–æ‰€æœ‰è§†é¢‘çš„å†å²è¯„è®º
    - ç›‘æ§çˆ¬è™«ï¼šå®šæœŸç›‘æ§æ–°å¢è¯„è®º
    - æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼å’Œè°ƒåº¦ç­–ç•¥

ç”¨æ³•ï¼š
    # å†å²çˆ¬è™«æ¨¡å¼
    python programs/run_crawler.py history --all
    python programs/run_crawler.py history --accounts 1,3

    # ç›‘æ§çˆ¬è™«æ¨¡å¼
    python programs/run_crawler.py monitor
    python programs/run_crawler.py monitor --top-n 10

    # æ··åˆæ¨¡å¼ï¼ˆå…ˆå†å²åç›‘æ§ï¼‰
    python programs/run_crawler.py hybrid --all

è®¾è®¡ç†å¿µï¼š
    - å•ä¸€å…¥å£ï¼šæ‰€æœ‰çˆ¬è™«åŠŸèƒ½ç»Ÿä¸€ç®¡ç†
    - æ¨¡å—åŒ–ï¼šå†å²å’Œç›‘æ§çˆ¬è™«é€»è¾‘ç‹¬ç«‹
    - å¯é…ç½®ï¼šæ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶
    - æ˜“æ‰©å±•ï¼šä¾¿äºæ·»åŠ æ–°çš„çˆ¬è™«ç­–ç•¥
"""

import sys
import logging
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import List, Optional

# è®¾ç½®é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_dir = PROJECT_ROOT / 'logs'
log_dir.mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'crawler_service.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

from src.database.manager import DatabaseManager
from src.crawler.history_crawler import HistoryCrawler
from src.crawler.monitor_crawler import MonitorCrawler
from src.generator.task_generator import TaskGenerator
from src.crawler.api_client import DouyinAPIClient


class CrawlerService:
    """ç»Ÿä¸€çš„çˆ¬è™«æœåŠ¡ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«æœåŠ¡"""
        self.db = None
        self.api_client = None
        self.task_generator = None
        self.config = {}

    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            # åŠ è½½é…ç½®
            logger.info("åŠ è½½é…ç½®...")
            self.config = self._load_config()

            # åˆå§‹åŒ–æ•°æ®åº“
            logger.info("åˆå§‹åŒ–æ•°æ®åº“...")
            self.db = DatabaseManager()
            self.db.init_db()
            logger.info("âœ“ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

            # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
            logger.info("åˆå§‹åŒ–APIå®¢æˆ·ç«¯...")
            self.api_client = DouyinAPIClient()
            logger.info("âœ“ APIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

            # åˆå§‹åŒ–ä»»åŠ¡ç”Ÿæˆå™¨
            self.task_generator = TaskGenerator(self.db)
            logger.info("âœ“ ä»»åŠ¡ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

            return True

        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error("""
å¯èƒ½çš„åŸå› :
  1. é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: config/config.json
  2. APIæœåŠ¡å™¨æ— æ³•è¿æ¥
  3. æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥

è§£å†³æ–¹æ¡ˆ:
  1. æ£€æŸ¥ config/config.json é…ç½®æ–‡ä»¶
  2. éªŒè¯APIæœåŠ¡å™¨åœ°å€å’Œå¯†é’¥
  3. ç¡®ä¿æ•°æ®åº“æ–‡ä»¶å¯è®¿é—®
            """)
            return False

    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶

        Returns:
            dict: é…ç½®å­—å…¸
        """
        try:
            config_file = PROJECT_ROOT / 'config' / 'config.json'
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning("âš  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
        except Exception as e:
            logger.warning(f"âš  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    def get_target_accounts(self, account_filter: Optional[str] = None) -> List:
        """è·å–ç›®æ ‡è´¦å·åˆ—è¡¨

        Args:
            account_filter: è´¦å·è¿‡æ»¤å™¨ï¼ˆå¦‚ "1,3" æˆ– "all"ï¼‰

        Returns:
            List: ç›®æ ‡è´¦å·åˆ—è¡¨
        """
        all_accounts = self.db.get_target_accounts()

        if not all_accounts:
            logger.error("âŒ æœªé…ç½®ä»»ä½•ç›®æ ‡è´¦å·")
            logger.info("è¯·åœ¨ config/target_accounts.json ä¸­é…ç½®ç›®æ ‡è´¦å·")
            return []

        logger.info(f"âœ“ æ‰¾åˆ° {len(all_accounts)} ä¸ªç›®æ ‡è´¦å·")

        # å¦‚æœæ²¡æœ‰è¿‡æ»¤å™¨ï¼Œè¿”å›æ‰€æœ‰è´¦å·
        if not account_filter or account_filter == 'all':
            return all_accounts

        # è§£æè´¦å·ç¼–å·è¿‡æ»¤
        try:
            indices = [int(x.strip()) for x in account_filter.split(',')]
            if any(i < 1 or i > len(all_accounts) for i in indices):
                logger.error(f"âŒ ç¼–å·è¶…å‡ºèŒƒå›´ï¼è¯·è¾“å…¥ 1-{len(all_accounts)} ä¹‹é—´çš„æ•°å­—")
                return []

            selected = [all_accounts[i-1] for i in sorted(set(indices))]
            logger.info(f"âœ“ å·²é€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected

        except ValueError:
            logger.error("âŒ è´¦å·ç¼–å·æ ¼å¼é”™è¯¯ï¼ç¤ºä¾‹ï¼š1,3")
            return []

    def run_history_crawler(self, accounts: List, days: int = 90) -> dict:
        """è¿è¡Œå†å²çˆ¬è™«

        Args:
            accounts: ç›®æ ‡è´¦å·åˆ—è¡¨
            days: çˆ¬å–æœ€è¿‘Nå¤©çš„æ•°æ®

        Returns:
            dict: çˆ¬è™«ç»“æœç»Ÿè®¡
        """
        logger.info("=" * 70)
        logger.info("ğŸ“ å†å²çˆ¬è™«æ¨¡å¼ - å¯åŠ¨")
        logger.info(f"   è´¦å·æ•°é‡: {len(accounts)}")
        logger.info(f"   æ—¶é—´èŒƒå›´: æœ€è¿‘ {days} å¤©")
        logger.info("=" * 70)

        crawler = HistoryCrawler(self.db, self.api_client)

        total_comments = 0
        total_videos = 0
        total_tasks = 0
        success_count = 0
        failed_count = 0

        for idx, account in enumerate(accounts, 1):
            logger.info(f"\n[è´¦å· {idx}/{len(accounts)}] {account.account_name}")
            logger.info("-" * 70)

            try:
                # çˆ¬å–å†å²è¯„è®º
                result = crawler.crawl_history(account, days=days)

                if result['status'] == 'success':
                    videos = result.get('total_videos', 0)
                    comments = result.get('total_comments', 0)

                    logger.info(f"  âœ“ çˆ¬è™«å®Œæˆ")
                    logger.info(f"    - è§†é¢‘æ•°: {videos}")
                    logger.info(f"    - è¯„è®ºæ•°: {comments}")

                    total_videos += videos
                    total_comments += comments
                    success_count += 1

                    # ç”Ÿæˆä»»åŠ¡
                    task_count = self.task_generator.generate_from_history(account.id)
                    total_tasks += task_count
                    logger.info(f"  âœ“ ä»»åŠ¡ç”Ÿæˆ: {task_count} ä¸ª")

                else:
                    logger.error(f"  âœ— çˆ¬è™«å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    failed_count += 1

            except Exception as e:
                logger.error(f"  âœ— å¤„ç†å¤±è´¥: {e}")
                failed_count += 1

        # è¿”å›ç»Ÿè®¡ç»“æœ
        stats = {
            'mode': 'history',
            'accounts_total': len(accounts),
            'accounts_success': success_count,
            'accounts_failed': failed_count,
            'total_videos': total_videos,
            'total_comments': total_comments,
            'total_tasks': total_tasks
        }

        self._print_summary(stats)
        return stats

    def run_monitor_crawler(self, accounts: List, top_n: int = 5) -> dict:
        """è¿è¡Œç›‘æ§çˆ¬è™«

        Args:
            accounts: ç›®æ ‡è´¦å·åˆ—è¡¨
            top_n: ç›‘æ§å‰Nä¸ªè§†é¢‘

        Returns:
            dict: ç›‘æ§ç»“æœç»Ÿè®¡
        """
        logger.info("=" * 70)
        logger.info("ğŸ‘ï¸  ç›‘æ§çˆ¬è™«æ¨¡å¼ - å¯åŠ¨")
        logger.info(f"   è´¦å·æ•°é‡: {len(accounts)}")
        logger.info(f"   ç›‘æ§ç­–ç•¥: æ¯è´¦å·å‰ {top_n} ä¸ªè§†é¢‘")
        logger.info(f"   è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 70)

        crawler = MonitorCrawler(self.db, self.api_client)

        total_new_comments = 0
        total_tasks = 0
        success_count = 0
        failed_count = 0

        for idx, account in enumerate(accounts, 1):
            logger.info(f"\n[è´¦å· {idx}/{len(accounts)}] {account.account_name}")
            logger.info("-" * 70)

            try:
                # ç›‘æ§æ–°å¢è¯„è®º
                result = crawler.monitor_daily(account, top_n=top_n)

                if result['status'] == 'success':
                    new_count = result.get('new_comments_count', 0)
                    logger.info(f"  âœ“ ç›‘æ§å®Œæˆ")
                    logger.info(f"    - æ–°å¢è¯„è®º: {new_count} æ¡")

                    total_new_comments += new_count
                    success_count += 1

                    # ç”Ÿæˆé«˜ä¼˜å…ˆçº§ä»»åŠ¡
                    if new_count > 0:
                        task_count = self.task_generator.generate_from_realtime(account.id)
                        total_tasks += task_count
                        logger.info(f"  âœ“ ç”Ÿæˆä»»åŠ¡: {task_count} ä¸ªï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰")
                    else:
                        logger.info(f"  â„¹ï¸  æš‚æ— æ–°å¢")

                else:
                    logger.error(f"  âœ— ç›‘æ§å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    failed_count += 1

            except Exception as e:
                logger.error(f"  âœ— å¤„ç†å¤±è´¥: {e}")
                failed_count += 1

        # è¿”å›ç»Ÿè®¡ç»“æœ
        stats = {
            'mode': 'monitor',
            'accounts_total': len(accounts),
            'accounts_success': success_count,
            'accounts_failed': failed_count,
            'new_comments': total_new_comments,
            'total_tasks': total_tasks
        }

        self._print_summary(stats)
        return stats

    def run_hybrid_crawler(self, accounts: List, days: int = 90, top_n: int = 5) -> dict:
        """è¿è¡Œæ··åˆæ¨¡å¼ï¼ˆå…ˆå†å²åç›‘æ§ï¼‰

        Args:
            accounts: ç›®æ ‡è´¦å·åˆ—è¡¨
            days: å†å²çˆ¬è™«å¤©æ•°
            top_n: ç›‘æ§å‰Nä¸ªè§†é¢‘

        Returns:
            dict: æ··åˆç»“æœç»Ÿè®¡
        """
        logger.info("=" * 70)
        logger.info("ğŸ”„ æ··åˆçˆ¬è™«æ¨¡å¼ - å¯åŠ¨")
        logger.info("   æ‰§è¡Œé¡ºåº: å†å²çˆ¬è™« â†’ ç›‘æ§çˆ¬è™«")
        logger.info("=" * 70)

        # å…ˆè¿è¡Œå†å²çˆ¬è™«
        logger.info("\nã€ç¬¬1é˜¶æ®µã€‘å†å²çˆ¬è™«")
        history_stats = self.run_history_crawler(accounts, days)

        # å†è¿è¡Œç›‘æ§çˆ¬è™«
        logger.info("\nã€ç¬¬2é˜¶æ®µã€‘ç›‘æ§çˆ¬è™«")
        monitor_stats = self.run_monitor_crawler(accounts, top_n)

        # åˆå¹¶ç»Ÿè®¡
        stats = {
            'mode': 'hybrid',
            'history': history_stats,
            'monitor': monitor_stats
        }

        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š æ··åˆæ¨¡å¼ - æ€»ä½“ç»Ÿè®¡")
        logger.info("=" * 70)
        logger.info(f"  å†å²è¯„è®ºæ•°: {history_stats['total_comments']}")
        logger.info(f"  æ–°å¢è¯„è®ºæ•°: {monitor_stats['new_comments']}")
        logger.info(f"  æ€»ä»»åŠ¡æ•°: {history_stats['total_tasks'] + monitor_stats['total_tasks']}")
        logger.info("=" * 70)

        return stats

    def _print_summary(self, stats: dict):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦

        Args:
            stats: ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        logger.info("\n" + "=" * 70)
        logger.info(f"ğŸ“Š {stats['mode'].upper()} æ¨¡å¼ - ç»Ÿè®¡ç»“æœ")
        logger.info("=" * 70)
        logger.info(f"  è´¦å·æ€»æ•°: {stats['accounts_total']}")
        logger.info(f"    - æˆåŠŸ: {stats['accounts_success']}")
        logger.info(f"    - å¤±è´¥: {stats['accounts_failed']}")

        if stats['mode'] == 'history':
            logger.info(f"  æ€»è§†é¢‘æ•°: {stats['total_videos']}")
            logger.info(f"  æ€»è¯„è®ºæ•°: {stats['total_comments']}")
            logger.info(f"  ç”Ÿæˆä»»åŠ¡: {stats['total_tasks']}")
        elif stats['mode'] == 'monitor':
            logger.info(f"  æ–°å¢è¯„è®º: {stats['new_comments']}")
            logger.info(f"  ç”Ÿæˆä»»åŠ¡: {stats['total_tasks']}")

        logger.info("=" * 70)


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='DY-Interaction ç»Ÿä¸€çˆ¬è™«æœåŠ¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å†å²çˆ¬è™« - çˆ¬å–æ‰€æœ‰è´¦å·
  python programs/run_crawler.py history --all

  # å†å²çˆ¬è™« - çˆ¬å–æŒ‡å®šè´¦å·
  python programs/run_crawler.py history --accounts 1,3

  # ç›‘æ§çˆ¬è™« - æ‰€æœ‰è´¦å·
  python programs/run_crawler.py monitor

  # ç›‘æ§çˆ¬è™« - è‡ªå®šä¹‰ç›‘æ§æ•°é‡
  python programs/run_crawler.py monitor --top-n 10

  # æ··åˆæ¨¡å¼ - å…ˆå†å²åç›‘æ§
  python programs/run_crawler.py hybrid --all
        """
    )

    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='mode', help='çˆ¬è™«æ¨¡å¼', required=True)

    # å†å²çˆ¬è™«æ¨¡å¼
    history_parser = subparsers.add_parser('history', help='å†å²çˆ¬è™«æ¨¡å¼')
    history_parser.add_argument('--all', action='store_true', help='çˆ¬å–æ‰€æœ‰è´¦å·')
    history_parser.add_argument('--accounts', type=str, help='æŒ‡å®šè´¦å·ç¼–å·ï¼ˆå¦‚ï¼š1,3ï¼‰')
    history_parser.add_argument('--days', type=int, default=90, help='çˆ¬å–æœ€è¿‘Nå¤©ï¼ˆé»˜è®¤90ï¼‰')

    # ç›‘æ§çˆ¬è™«æ¨¡å¼
    monitor_parser = subparsers.add_parser('monitor', help='ç›‘æ§çˆ¬è™«æ¨¡å¼')
    monitor_parser.add_argument('--all', action='store_true', help='ç›‘æ§æ‰€æœ‰è´¦å·ï¼ˆé»˜è®¤ï¼‰')
    monitor_parser.add_argument('--accounts', type=str, help='æŒ‡å®šè´¦å·ç¼–å·ï¼ˆå¦‚ï¼š1,3ï¼‰')
    monitor_parser.add_argument('--top-n', type=int, default=5, help='ç›‘æ§å‰Nä¸ªè§†é¢‘ï¼ˆé»˜è®¤5ï¼‰')

    # æ··åˆæ¨¡å¼
    hybrid_parser = subparsers.add_parser('hybrid', help='æ··åˆæ¨¡å¼ï¼ˆå†å²+ç›‘æ§ï¼‰')
    hybrid_parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰è´¦å·')
    hybrid_parser.add_argument('--accounts', type=str, help='æŒ‡å®šè´¦å·ç¼–å·ï¼ˆå¦‚ï¼š1,3ï¼‰')
    hybrid_parser.add_argument('--days', type=int, default=90, help='å†å²çˆ¬è™«å¤©æ•°ï¼ˆé»˜è®¤90ï¼‰')
    hybrid_parser.add_argument('--top-n', type=int, default=5, help='ç›‘æ§è§†é¢‘æ•°ï¼ˆé»˜è®¤5ï¼‰')

    return parser


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = create_parser()
    args = parser.parse_args()

    logger.info("=" * 70)
    logger.info("ğŸš€ DY-Interaction ç»Ÿä¸€çˆ¬è™«æœåŠ¡")
    logger.info(f"   æ¨¡å¼: {args.mode.upper()}")
    logger.info(f"   å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)

    try:
        # åˆå§‹åŒ–æœåŠ¡
        service = CrawlerService()
        if not service.initialize():
            return 1

        # è·å–ç›®æ ‡è´¦å·
        account_filter = 'all' if args.all else args.accounts
        accounts = service.get_target_accounts(account_filter)

        if not accounts:
            logger.error("âŒ æ²¡æœ‰å¯å¤„ç†çš„è´¦å·")
            return 1

        # æ ¹æ®æ¨¡å¼æ‰§è¡Œçˆ¬è™«
        if args.mode == 'history':
            service.run_history_crawler(accounts, args.days)

        elif args.mode == 'monitor':
            service.run_monitor_crawler(accounts, args.top_n)

        elif args.mode == 'hybrid':
            service.run_hybrid_crawler(accounts, args.days, args.top_n)

        logger.info("\nâœ… çˆ¬è™«æœåŠ¡æ‰§è¡Œå®Œæˆ")
        return 0

    except KeyboardInterrupt:
        logger.info("\n\nâ¸ï¸  æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        return 0

    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)
